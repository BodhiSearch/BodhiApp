- alias: llama3:instruct
  family: llama3
  repo: QuantFactory/Meta-Llama-3-8B-Instruct-GGUF
  filename: Meta-Llama-3-8B-Instruct.Q8_0.gguf
  features:
    - chat
  chat_template: llama3
  request_params:
    stop:
      - <|start_header_id|>
      - <|end_header_id|>
      - <|eot_id|>
  context_params:
    n_parallel: 4
    n_keep: 24
- alias: llama3:70b-instruct
  family: llama3
  repo: QuantFactory/Meta-Llama-3-70B-Instruct-GGUF
  filename: Meta-Llama-3-70B-Instruct.Q4_0.gguf
  features:
    - chat
  chat_template: llama3
  context_params:
    n_parallel: 1
    n_keep: 24
  request_params:
    stop:
      - <|start_header_id|>
      - <|end_header_id|>
      - <|eot_id|>
- alias: llama2:chat
  family: llama2
  repo: TheBloke/Llama-2-7B-Chat-GGUF
  filename: llama-2-7b-chat.Q8_0.gguf
  features:
    - chat
  chat_template: llama2
  context_params:
    n_parallel: 4
  request_params:
    stop:
      - "[INST]"
      - "[/INST]"
      - "<<SYS>>"
      - "<</SYS>>"
- alias: llama2:13b-chat
  family: llama2
  repo: TheBloke/Llama-2-13B-chat-GGUF
  filename: llama-2-13b-chat.Q8_0.gguf
  features:
    - chat
  chat_template: llama2
  context_params:
    n_parallel: 4
  request_params:
    stop:
      - "[INST]"
      - "[/INST]"
      - "<<SYS>>"
      - "<</SYS>>"
- alias: llama2:70b-chat
  family: llama2
  repo: TheBloke/Llama-2-70B-Chat-GGUF
  filename: llama-2-70b-chat.Q4_0.gguf
  features:
    - chat
  chat_template: llama2
  context_params:
    n_parallel: 1
  request_params:
    stop:
      - "[INST]"
      - "[/INST]"
      - "<<SYS>>"
      - "<</SYS>>"
- alias: phi3:mini
  family: phi3
  repo: microsoft/Phi-3-mini-4k-instruct-gguf
  filename: Phi-3-mini-4k-instruct-fp16.gguf
  features:
    - chat
  chat_template: phi3
  context_params:
    n_parallel: 4
  request_params:
    stop:
      - <|end|>
      - <|user|>
      - <|assistant|>
- alias: mistral:instruct
  family: mistral
  repo: MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF
  filename: Mistral-7B-Instruct-v0.3.Q8_0.gguf
  features:
    - chat
  chat_template: llama2-legacy
  context_params:
    n_parallel: 4
  request_params:
    stop:
      - "[INST]"
      - "[/INST]"
- alias: mixtral:instruct
  family: mixtral
  repo: TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF
  filename: mixtral-8x7b-instruct-v0.1.Q8_0.gguf
  features:
    - chat
  chat_template: llama2-legacy
  context_params:
    n_parallel: 1
  request_params:
    stop:
      - "[INST]"
      - "[/INST]"
- alias: gemma:instruct
  family: gemma
  repo: google/gemma-7b-it-GGUF
  filename: gemma-7b-it.gguf
  features:
    - chat
  chat_template: gemma
  context_params:
    n_parallel: 4
- alias: gemma:7b-instruct-v1.1-q8_0
  family: gemma
  repo: google/gemma-1.1-7b-it-GGUF
  filename: 7b_it_v1p1.gguf
  features:
    - chat
  chat_template: gemma
  context_params:
    n_parallel: 4
- alias: tinyllama:instruct
  family: tinyllama
  repo: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF
  filename: tinyllama-1.1b-chat-v1.0.Q4_0.gguf
  features:
    - chat
  chat_template: tinyllama
  request_params:
    frequency_penalty: 0.0
    max_tokens: 256
    presence_penalty: 0.0
    stop:
      - <|system|>
      - <|user|>
      - <|assistant|>
      - </s>
    temperature: 0.7
    top_p: 0.95
  context_params:
    n_seed: 42
    n_ctx: 2048
    n_parallel: 4
    n_predict: 256
    n_keep: 4
