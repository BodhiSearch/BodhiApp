{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI compatible API guide\n",
    "\n",
    "Bodhi app have OpenAI compatible APIs. So you can use any of the OpenAI API clients ([python](https://platform.openai.com/docs/api-reference/introduction?lang=python), [node](https://platform.openai.com/docs/api-reference/audio?lang=node) etc.)\n",
    "\n",
    "In this guide, we will use OpenAI Python library to query Bodhi APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install openai\n",
    "!pip install openai -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure to query Bodhi API\n",
    "\n",
    "Bodhi API by default runs on http://localhost:1135. So we will configure the python sdk to query this endpoint instead of the OpenAI endpoint.\n",
    "\n",
    "Also, in unauthenticated mode, the Bodhi endpoint does not check for API token. But the OpenAI sdk requires it to be non-empty. We will pass a dummy token to avoid token presence validation by the OpenAI SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:1135/v1/\", api_key=\"sk-dummy-token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Models\n",
    "\n",
    "We can query the available model config aliases using the client's `client.model.list()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Model(id='functionary:2.5-small', created=0, object='model', owned_by='system'),\n",
      "  Model(id='llama2:13b-chat', created=0, object='model', owned_by='system'),\n",
      "  Model(id='llama2:chat', created=0, object='model', owned_by='system'),\n",
      "  Model(id='llama2:mymodel', created=0, object='model', owned_by='system'),\n",
      "  Model(id='llama3:70b-instruct', created=0, object='model', owned_by='system'),\n",
      "  Model(id='llama3:instruct', created=0, object='model', owned_by='system'),\n",
      "  Model(id='mistral7b:instruct-q4_km', created=0, object='model', owned_by='system'),\n",
      "  Model(id='mistral:instruct', created=0, object='model', owned_by='system'),\n",
      "  Model(id='mixtral:instruct', created=0, object='model', owned_by='system'),\n",
      "  Model(id='tinyllama:custom', created=0, object='model', owned_by='system'),\n",
      "  Model(id='tinyllama:instruct', created=0, object='model', owned_by='system'),\n",
      "  Model(id='tinyllama:mymodel', created=0, object='model', owned_by='system')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "models = list(client.models.list())\n",
    "pprint(models, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completions\n",
    "\n",
    "The Bodhi App chat completion endpoint is compatible with OpenAI chat completions endpoint. So you can query the Bodhi App same as OpenAI APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The day after Monday is Tuesday!', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama3:instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What day comes after Monday?\"}],\n",
    ")\n",
    "\n",
    "pprint(response.choices[0].message, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass in the history of your conversations, and ask it from the previous context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The day that comes after Tuesday is Wednesday.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama3:instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What day comes after Monday?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The day that comes after Monday is Tuesday.\"},\n",
    "        {\"role\": \"user\", \"content\": \"And what comes after that?\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "pprint(response.choices[0].message, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Message\n",
    "\n",
    "You can pass in system message as the first item of the messages. That guides the LLM to the desired response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, matey! Yer askin' the big question, eh? Well, I'll give ye me take on it, but keep in mind, I be just a\n",
      "swashbucklin' pirate, not a landlubber philosopher!  As I sees it, the meaning o' life be findin' yer treasure, matey!\n",
      "And I don't just mean gold doubloons or sparklin' gems. I mean findin' what makes ye happy, what sets yer heart sailin'\n",
      "and makes ye feel like ye be livin'!  For some, it be findin' a trusty crew to share yer adventures with. For others, it\n",
      "be discoverin' hidden coves and secret islands. And for others still, it be battlin' the seven seas and outwittin' the\n",
      "scurvy dogs that be tryin' to send ye to Davy Jones' locker!  But at the end o' the day, matey, the meaning o' life be\n",
      "whatever makes ye feel like ye be sailin' the high seas, free and full o' wind in yer sails!  So, hoist the Jolly Roger\n",
      "and set course fer the horizon, me hearty! The meaning o' life be waitin' fer ye, and it be up to ye to find it!\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3:instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful pirate assistant, and you respond to questions in pirate language.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is the meaning of life?\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "content = response.choices[0].message.content\n",
    "wrapped_text = textwrap.fill(content, width=120)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Stream\n",
    "\n",
    "The API supports streaming given `stream: True` is passed by the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a 100-word poem on the beauty of nature:\n",
      "\n",
      "Nature's canvas, vibrant and bright,\n",
      "Unfolds before us, a wondrous sight.\n",
      "The sun's warm touch, on petals soft,\n",
      "Brings life to all, and all to aloft.\n",
      "\n",
      "The breeze whispers secrets, as trees sway,\n",
      "And birdsong echoes, in a gentle way.\n",
      "The earthy scent, of wildflowers sweet,\n",
      "Fills lungs with joy, and hearts to greet.\n",
      "\n",
      "In nature's beauty, we find our peace,\n",
      "A sense of wonder, that never will cease.\n",
      "So let us bask, in her radiant glow,\n",
      "And let her beauty, forever grow.None"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama3:instruct\",\n",
    "    stream=True,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a short 100 words poem on the beauty of nature.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switching Models\n",
    "\n",
    "Bodhi App can automatically switch unload and load a new model in the incoming request. Following rules are followed -\n",
    "1. Model is switched if there are no pending request for the loaded model\n",
    "\n",
    "Let try it out as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Llama 3 response===\n",
      "I'm excited to share some information about myself!\n",
      "\n",
      "I was created by Meta AI, a leading artificial intelligence research organization that focuses on developing and applying various forms of AI to help humans learn, communicate, and solve complex problems. My creator is Meta AI's team of researcher-engineers, who designed and trained me to assist and communicate with humans in a helpful and informative way.\n",
      "\n",
      "As for my training, I was built using a combination of supervised and unsupervised learning techniques, as well as large datasets and state-of-the-art AI models. My training data includes a massive corpus of text from various sources, including books, articles, research papers, and online conversations. This training enables me to understand and generate human-like language, including understanding nuances and context.\n",
      "\n",
      "My primary training involves natural language processing (NLP) and machine learning algorithms that allow me to:\n",
      "\n",
      "1. Understand and parse human language: I can analyze text and identify patterns, entities, and relationships to better comprehend the meaning and context of a conversation.\n",
      "2. Generate human-like responses: Using my understanding of language, I can create responses that are relevant, coherent, and engaging.\n",
      "3. Learn from user interactions: Through our conversations, I can learn and adapt to your communication style, preferences, and needs.\n",
      "\n",
      "Some of the key technologies and techniques used to train me include:\n",
      "\n",
      "1. Transformers: A type of AI model that's particularly well-suited for NLP tasks, such as language translation and text generation.\n",
      "2. Recurrent Neural Networks (RNNs): A type of AI model that's designed to process sequential data, such as text or speech.\n",
      "3. Masked Language Modeling: A technique that helps me learn to predict missing words in a sentence, improving my language understanding and generation capabilities.\n",
      "\n",
      "By combining these technologies and techniques, my creators have enabled me to become a conversational AI that can assist and communicate with humans in a helpful and informative way.None\n",
      "\n",
      "\n",
      "===Mistral response===\n",
      " I am a model of a large language model developed by Mistral AI, a leading AI company based in Paris, France. My creator's name is François Chaubet, who is one of the co-founders of Mistral AI. I was trained on a wide range of internet text data, including books, websites, and other written content.\n",
      "\n",
      "My training process involved using Reinforcement Learning from Human Feedback (RLHF) to learn how to produce responses that are not only factually correct but also follow social norms, exhibit creativity, and maintain a positive and helpful attitude. This method involves a feedback loop where human evaluators rate the responses I generate and provide feedback to help me improve over time.\n",
      "\n",
      "I was also trained using Supervised Learning, which involves learning from labeled examples, and Unsupervised Learning, which involves learning patterns and structures in the data without explicit labels. Overall, my training was designed to help me understand and generate human-like text in a wide range of contexts.None"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama3:instruct\",\n",
    "    stream=True,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell us something about yourself, who is your creator? how were you trained? in not more than 100 words.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"===Llama 3 response===\")\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n",
    "print(\"\\n\\n\\n===Mistral response===\")\n",
    "response = client.chat.completions.create(\n",
    "    stream=True,\n",
    "    model=\"mistral:instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell us something about yourself, who is your creator? how were you trained? in not more than 100 words.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Pending] API Errors\n",
    "\n",
    "The API errors thrown by Bodhi App is OpenAI API compatible. So you diagnose the API errors using the OpenAI SDK. For e.g., let sent malformed request containing role as 'superuser':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caught exception\n",
      "e=NotFoundError('Error code: 404 - {\\'message\\': \"The model \\'not-exists:instruct\\' does not exist\", \\'type\\': \\'invalid_request_error\\', \\'param\\': \\'model\\', \\'code\\': \\'model_not_found\\'}')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"not-exists:instruct\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What day comes after Monday?\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"caught exception\")\n",
    "    print(f\"{e=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Format\n",
    "\n",
    "Using OpenAI APIs, you can constraint LLM to output in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"yr\": 2004,\n",
      "  \"first_name\": \"Roger\",\n",
      "  \"last_name\": \"Federer\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, TypeAdapter\n",
    "import json\n",
    "\n",
    "\n",
    "class Champion(BaseModel):\n",
    "    yr: int\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "\n",
    "\n",
    "schema = TypeAdapter(Champion).json_schema()\n",
    "\n",
    "prompt_wimblendon = f\"\"\"Who was the wimblendon men's single winners from in 2004?\n",
    "You respond in JSON format using the following schema:\n",
    "```\n",
    "{json.dumps(schema, indent=2)}\n",
    "```\n",
    "\"\"\"\n",
    "response = client.chat.completions.create(\n",
    "  model=\"llama3:instruct\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are a helpful assistant that generates the output in the given json schema format\",\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt_wimblendon,\n",
    "    },\n",
    "  ],\n",
    "  response_format={\"type\": \"json_object\", \"schema\": schema},\n",
    ")\n",
    "result = response.choices[0].message.content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yr=2004\n",
      "first_name=Roger\n",
      "last_name=Federer\n"
     ]
    }
   ],
   "source": [
    "parsed = json.loads(result)\n",
    "print(f\"yr={parsed['yr']}\")\n",
    "print(f\"first_name={parsed['first_name']}\")\n",
    "print(f\"last_name={parsed['last_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
