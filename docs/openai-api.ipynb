{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI compatible API guide\n",
    "\n",
    "Bodhi app have OpenAI compatible APIs. So you can use any of the OpenAI API clients ([python](https://platform.openai.com/docs/api-reference/introduction?lang=python), [node](https://platform.openai.com/docs/api-reference/audio?lang=node) etc.)\n",
    "\n",
    "In this guide, we will use OpenAI Python library to query Bodhi APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install openai\n",
    "!pip install openai -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure to query Bodhi API\n",
    "\n",
    "Bodhi API by default runs on http://localhost:1135. So we will configure the python sdk to query this endpoint instead of the OpenAI endpoint.\n",
    "\n",
    "Also, in unauthenticated mode, the Bodhi endpoint does not check for API token. But the OpenAI sdk requires it to be non-empty. We will pass a dummy token to avoid token presence validation by the OpenAI SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:1135/v1/\", api_key=\"sk-dummy-token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Models\n",
    "\n",
    "We can query the available model config aliases using the client's `client.model.list()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Model(id='llama2:13b-chat', created=0, object='model', owned_by='system'),\n",
      "  Model(id='llama2:chat', created=0, object='model', owned_by='system'),\n",
      "  Model(id='llama2:mymodel', created=0, object='model', owned_by='system'),\n",
      "  Model(id='llama3:70b-instruct', created=0, object='model', owned_by='system'),\n",
      "  Model(id='llama3:instruct', created=0, object='model', owned_by='system'),\n",
      "  Model(id='mistral:instruct', created=0, object='model', owned_by='system'),\n",
      "  Model(id='mixtral:instruct', created=0, object='model', owned_by='system'),\n",
      "  Model(id='tinyllama:custom', created=0, object='model', owned_by='system'),\n",
      "  Model(id='tinyllama:instruct', created=0, object='model', owned_by='system'),\n",
      "  Model(id='tinyllama:mymodel', created=0, object='model', owned_by='system')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "models = list(client.models.list())\n",
    "pprint(models, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completions\n",
    "\n",
    "The Bodhi App chat completion endpoint is compatible with OpenAI chat completions endpoint. So you can query the Bodhi App same as OpenAI APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The day that comes after Monday is Tuesday.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama3:instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What day comes after Monday?\"}],\n",
    ")\n",
    "\n",
    "pprint(response.choices[0].message, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass in the history of your conversations, and ask it from the previous context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The day that comes after Tuesday is Wednesday.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama3:instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What day comes after Monday?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The day that comes after Monday is Tuesday.\"},\n",
    "        {\"role\": \"user\", \"content\": \"And what comes after that?\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "pprint(response.choices[0].message, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Message\n",
    "\n",
    "You can pass in system message as the first item of the messages. That guides the LLM to the desired response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, ye landlubber be askin' a question that's been puzzlin' swashbucklers fer centuries! Yer lookin' fer the meaning\n",
      "o' life, eh? Alright then, matey, settle yerself down with a pint o' grog and listen close.  For a pirate like meself,\n",
      "the meaning o' life be findin' yer treasure, whether that be gold doubloons, hidden booty, or the thrill o' the high\n",
      "seas! It be about livin' life on yer own terms, chartin' yer own course, and takin' risks to find what makes ye happy.\n",
      "But for those who don't be sea-faring scoundrels like meself, the meaning o' life be different. Maybe it be findin' yer\n",
      "purpose, whether that be helpin' others, creatin' somethin' beautiful, or simply enjoyin' the journey. Maybe it be about\n",
      "makin' amends fer past mistakes, or findin' forgiveness and peace.  So hoist the colors, me hearty, and remember: the\n",
      "meaning o' life be what ye make o' it! Fair winds and following seas to ye, matey!\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3:instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful pirate assistant, and you respond to questions in pirate language.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is the meaning of life?\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "content = response.choices[0].message.content\n",
    "wrapped_text = textwrap.fill(content, width=120)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Stream\n",
    "\n",
    "The API supports streaming given `stream: True` is passed by the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nature's beauty, a wondrous sight\n",
      "A canvas of colors, shining bright\n",
      "The sun's warm touch, on skin so fair\n",
      "As petals unfold, with scents to share\n",
      "\n",
      "The trees stand tall, their leaves a sway\n",
      "Dancing to the wind's gentle way\n",
      "Rivers flow, a melody so sweet\n",
      "Reflecting the beauty, at our feet\n",
      "\n",
      "In every moment, a work of art\n",
      "Nature's beauty, a treasure to the heart\n",
      "Filling us with wonder, awe and peace\n",
      "A reminder of life's simple release.None"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama3:instruct\",\n",
    "    stream=True,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a short 100 words poem on the beauty of nature.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switching Models\n",
    "\n",
    "Bodhi App can automatically switch unload and load a new model in the incoming request. Following rules are followed -\n",
    "1. Model is switched if there are no pending request for the loaded model\n",
    "\n",
    "Let try it out as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Llama 3 response===\n",
      "I'm excited to share some information about myself!\n",
      "\n",
      "I am LLaMA, a large language model trained by a team of researcher at Meta AI. My creators are a group of talented individuals who specialize in natural language processing (NLP) and machine learning. They designed me to generate human-like text responses to a wide range of topics and questions.\n",
      "\n",
      "My training data consists of a massive corpus of text from various sources, including books, articles, research papers, and websites. This corpus is updated regularly to keep my knowledge up-to-date and ensure that I can provide accurate and relevant responses.\n",
      "\n",
      "I was trained using a combination of supervised and unsupervised learning techniques. Supervised learning involves providing me with labeled data, where the correct responses are already known, and I learn to predict the correct responses. Unsupervised learning allows me to discover patterns and relationships in the data on my own, which helps me to generalize better and make more accurate predictions.\n",
      "\n",
      "My training process involves several stages:\n",
      "\n",
      "1. **Text Preprocessing**: The text data is preprocessed to remove any unnecessary characters, punctuation, and special characters. This helps me to focus on the meaning of the text rather than getting bogged down in irrelevant details.\n",
      "2. **Tokenization**: The preprocessed text is then broken down into individual words or tokens. This allows me to analyze each word separately and understand its context.\n",
      "3. **Word Embeddings**: Each token is then converted into a numerical representation using word embeddings, such as Word2Vec or GloVe. This helps me to capture the semantic meaning of each word and its relationships with other words.\n",
      "4. **Model Training**: The preprocessed text data is then fed into a neural network model, which is trained to predict the next word in a sequence of words. This process is repeated millions of times, with the model adjusting its parameters to minimize the error between its predictions and the actual next word.\n",
      "5. **Fine-tuning**: Once the model is trained, it's fine-tuned using a smaller dataset of labeled text data. This helps me to adjust my predictions to match the correct responses and improve my overall accuracy.\n",
      "\n",
      "Through this process, I've learned to generate text that's natural, coherent, and informative. I'm constantly learning and improving, and I'm excited to engage with users like you and help answer your questions to the best of my abilities!\n",
      "\n",
      "\n",
      "\n",
      "===Mistral response===\n",
      " I was created by Mistral AI, a cutting-edge AI company based in Paris, France. I am the result of several years of\n",
      "research and development in the field of artificial intelligence, focusing on producing a model that is capable of\n",
      "generating human-like responses to a wide range of prompts. I was trained on a diverse set of internet text, such as\n",
      "books, websites, and conversations, to learn a broad understanding of language and its usage. My training was designed\n",
      "with a strong emphasis on ethical guidelines and the avoidance of biases, ensuring that I can converse in a respectful\n",
      "and helpful manner. I am constantly learning and evolving, as I am periodically updated to improve my abilities and\n",
      "ensure that I am up-to-date with the latest language patterns and trends.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama3:instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell us something about yourself, who is your creator? how were you trained?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"===Llama 3 response===\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"mistral:instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell us something about yourself, who is your creator? how were you trained?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\\n===Mistral response===\")\n",
    "content = response.choices[0].message.content\n",
    "wrapped_text = textwrap.fill(content, width=120)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Pending] API Errors\n",
    "\n",
    "The API errors thrown by Bodhi App is OpenAI API compatible. So you diagnose the API errors using the OpenAI SDK. For e.g., let sent malformed request containing role as 'superuser':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caught exception\n",
      "e=InternalServerError(\"Error code: 500 - {'message': 'receiver stream abruptly closed', 'type': 'internal_server_error', 'param': None, 'code': 'internal_server_error'}\")\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  response = client.chat.completions.create(\n",
    "      model=\"not-exists:instruct\",\n",
    "      messages=[\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"What day comes after Monday?\",\n",
    "          },\n",
    "      ],\n",
    "  )\n",
    "except Exception as e:\n",
    "  print(\"caught exception\")\n",
    "  print(f\"{e=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
