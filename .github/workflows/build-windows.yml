name: Windows Build Flow

on:
  push:
    branches:
      - main
  workflow_dispatch:

env:
  CI: true
  RUST_BACKTRACE: 1

jobs:
  build-windows:
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        include:
          - platform: windows-latest
            target: x86_64-pc-windows-msvc
    runs-on: ${{ matrix.platform }}
    steps:
      - name: Set environment variables
        shell: pwsh
        run: |
          $RUN_URL = "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          echo "RUN_URL=${RUN_URL}" >> $env:GITHUB_ENV
          echo "TARGET=${{ matrix.target }}" >> $env:GITHUB_ENV
          $USER_HOME = $HOME
          echo "USER_HOME=${USER_HOME}" >> $env:GITHUB_ENV
      - name: Enable symlinks
        shell: pwsh
        run: |
          git config --global core.symlinks true
          git config --global core.autocrlf false
          git config --global core.eol lf
      - uses: actions/checkout@v4
      - name: Setup Git with PAT
        shell: pwsh
        run: |
          git config --global url.https://gh_pat:${{ secrets.GH_PAT }}@github.com/.insteadOf git@github.com:
          git submodule sync --recursive
          git submodule update --init --recursive --depth=1
          git config --global --unset url.https://gh_pat:${{ secrets.GH_PAT }}@github.com/.insteadOf

      - name: Cache HuggingFace models
        uses: actions/cache@v4
        id: cache-hf
        with:
          path: ${{ env.USER_HOME }}\.cache\huggingface
          key: hf-cache-Windows-llama2-7b-chat

      - name: Check and Download Llama model
        if: steps.cache-hf.outputs.cache-hit != 'true'
        shell: pwsh
        run: |
          $HF_PATH = Join-Path $env:USERPROFILE ".cache\huggingface"
          $SNAPSHOT_DIR = Join-Path $HF_PATH "hub\models--TheBloke--Llama-2-7B-Chat-GGUF\snapshots\191239b3e26b2882fb562ffccdd1cf0f65402adb"

          New-Item -ItemType Directory -Force -Path $SNAPSHOT_DIR

          $MODEL_PATH = Join-Path $SNAPSHOT_DIR "llama-2-7b-chat.Q4_K_M.gguf"
          if (-not (Test-Path $MODEL_PATH)) {
            $MODEL_URL = "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/191239b3e26b2882fb562ffccdd1cf0f65402adb/llama-2-7b-chat.Q4_K_M.gguf?download=true"
            curl.exe -L $MODEL_URL -o $MODEL_PATH
          }

      - name: Windows Setup
        uses: ./.github/actions/setup-win
      - name: setup-rust
        uses: ./.github/actions/setup-rust
        with:
          platform: ${{ matrix.platform }}
          target: ${{ matrix.target }}
      - name: setup-node
        uses: ./.github/actions/setup-node
        with:
          platform: ${{ matrix.platform }}

      - name: Clean
        shell: pwsh
        run: |
          make -f Makefile.win.mk ci.clean

      - name: Generate code coverage
        shell: pwsh
        run: make -f Makefile.win.mk ci.coverage
        env:
          HF_TEST_TOKEN_ALLOWED: ${{ secrets.HF_TEST_TOKEN_ALLOWED }}
          HF_TEST_TOKEN_PUBLIC: ${{ secrets.HF_TEST_TOKEN_PUBLIC }}

      - name: Upload coverage reports to Codecov
        uses: codecov/codecov-action@v4.0.1
        with:
          name: BodhiApp-${{ matrix.target }}
          token: ${{ secrets.CODECOV_TOKEN }}
          slug: BodhiSearch/BodhiApp
          files: lcov.info
          fail_ci_if_error: true
          flags: ${{ matrix.target }},bodhi
