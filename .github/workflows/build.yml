name: Mac/Linux Build Flow

on:
  push:
    branches:
      - main
  workflow_dispatch:

env:
  CI: true
  RUST_BACKTRACE: 1

jobs:
  build:
    timeout-minutes: 40
    strategy:
      fail-fast: false
      matrix:
        include:
          - platform: macos-latest
            target: aarch64-apple-darwin
          - platform: ubuntu-latest
            target: x86_64-unknown-linux-gnu
          # - platform: windows-latest
          #   target: x86_64-pc-windows-msvc
          # - platform: macos-13
          #   target: x86_64-apple-darwin
    runs-on: ${{ matrix.platform }}
    steps:
      - name: envs
        shell: bash
        run: |
          echo "USER_HOME=$HOME" >> $GITHUB_ENV

      - name: Enable symlinks
        shell: bash
        run: |
          git config --global core.symlinks true
          git config --global core.autocrlf false
          git config --global core.eol lf
      - uses: actions/checkout@v4
      - name: Rewrite submodule URLs and checkout submodules
        shell: bash
        run: |
          git config --global url.https://gh_pat:${{ secrets.GH_PAT }}@github.com/.insteadOf git@github.com:
          git submodule sync --recursive
          git submodule update --init --recursive --depth=1
          git config --global --unset url.https://gh_pat:${{ secrets.GH_PAT }}@github.com/.insteadOf

      - name: Cache HuggingFace models
        uses: actions/cache@v4
        id: cache-hf
        with:
          path: ${{ env.USER_HOME }}/.cache/huggingface
          key: hf-cache-llama2-7b-chat
          enableCrossOsArchive: true

      - name: Check and Download Llama model
        if: steps.cache-hf.outputs.cache-hit != 'true'
        run: |
          python -m pip install -U pip
          python -m pip install -r crates/objs/tests/scripts/requirements.txt
          pip install -U "huggingface_hub[cli]"
          huggingface-cli download --revision 191239b3e26b2882fb562ffccdd1cf0f65402adb TheBloke/Llama-2-7B-Chat-GGUF llama-2-7b-chat.Q4_K_M.gguf
          huggingface-cli download --revision main --token ${{ secrets.CI_HF_TOKEN }} meta-llama/Llama-2-13b-chat-hf tokenizer_config.json

      - name: setup-rust
        uses: ./.github/actions/setup-rust
        with:
          platform: ${{ matrix.platform }}
          target: ${{ matrix.target }}
      - name: setup-node
        uses: ./.github/actions/setup-node
        with:
          platform: ${{ matrix.platform }}
      - name: clean
        shell: bash
        run: |
          make ci.clean
      - name: Generate code coverage
        shell: bash
        run: |
          make ci.coverage
        env:
          HF_TEST_TOKEN_ALLOWED: ${{ secrets.HF_TEST_TOKEN_ALLOWED }}
          HF_TEST_TOKEN_PUBLIC: ${{ secrets.HF_TEST_TOKEN_PUBLIC }}
          CI_DEFAULT_VARIANT: cpu
      - name: Upload coverage reports to Codecov
        uses: codecov/codecov-action@v4.0.1
        if: success()
        continue-on-error: true
        with:
          name: BodhiApp-${{ matrix.target }}
          token: ${{ secrets.CODECOV_TOKEN }}
          slug: BodhiSearch/BodhiApp
          files: lcov.info
          fail_ci_if_error: false
          flags: ${{ matrix.target }},bodhi
