<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# Optimizing llama-server for Microsoft Phi-4 on RTX A4000

Based on my comprehensive research of the latest llama.cpp documentation and performance optimization strategies, your **2-3 tokens per second performance is severely suboptimal** and can be dramatically improved. The primary issue is that you're likely running the model primarily on CPU rather than fully utilizing your RTX A4000 GPU.

## Critical Performance Issues Identified

Your current configuration is missing several **critical optimization parameters** that are essential for GPU-accelerated inference:

- **No GPU layer offloading** (`-ngl` parameter missing)
- **No Flash Attention** optimization
- **Default small batch sizes** limiting throughput
- **Unoptimized KV cache** consuming excessive VRAM
- **Suboptimal threading** configuration


## Optimized llama-server Configuration

Here's the dramatically improved configuration for your setup:

```bash
llama-server \
  --model <path-to-phi-4-Q4_K_M.gguf> \
  --alias phi-4-optimized \
  --api-key <your-api-key> \
  --port <your-port> \
  --jinja \
  -ngl -1 \
  -c 8192 \
  -b 2048 \
  -ub 512 \
  -fa \
  --cache-type-k q8_0 \
  --cache-type-v q8_0 \
  -t 4 \
  -tb 8 \
  --cont-batching \
  --parallel 2 \
  --mmap \
  --host 0.0.0.0
```


## Critical Parameter Explanations

### **GPU Offloading (Most Important)**

- **`-ngl -1`**: Offloads ALL model layers to GPU instead of CPU processing
- **Impact**: This single parameter should provide **5-10x performance improvement**
- **Why critical**: Your RTX A4000 has 16GB VRAM, easily sufficient for the ~8.5GB Phi-4 Q4_K_M model


### **Flash Attention Optimization**

- **`-fa`**: Enables Flash Attention kernels for memory-efficient attention computation[^1_1][^1_2]
- **Impact**: Reduces VRAM usage and improves performance significantly on Ada Lovelace architecture (A4000)
- **Benefit**: More efficient memory access patterns, critical for your GPU architecture


### **Batch Size Optimization**

- **`-b 2048`**: Logical batch size for parallel processing[^1_3][^1_4]
- **`-ub 512`**: Physical batch size balancing memory and performance[^1_5]
- **Impact**: Dramatically improves throughput, especially for longer sequences


### **KV Cache Quantization**

- **`--cache-type-k q8_0`** and **`--cache-type-v q8_0`**: Quantizes KV cache to 8-bit[^1_6]
- **VRAM savings**: ~3GB reduction (from 6GB to 3GB for 8K context)
- **Quality impact**: Minimal with Q8_0 quantization
- **Allows**: Larger context sizes or better performance headroom


## Expected Performance Improvements

| Configuration | Performance | VRAM Usage |
| :-- | :-- | :-- |
| **Current (CPU-focused)** | 2-3 tokens/sec | ~8GB |
| **Optimized (GPU-focused)** | **15-25+ tokens/sec** | ~11.5GB |
| **Performance gain** | **5-10x improvement** | +3.5GB efficiency |

## Memory Analysis for Your Setup

- **Model Size (Q4_K_M)**: ~8.5GB
- **KV Cache (default F16)**: ~6GB for 8K context
- **KV Cache (Q8_0 optimized)**: ~3GB for 8K context
- **Total optimized VRAM**: ~11.5GB (fits comfortably in 16GB)
- **Available headroom**: 4.5GB for larger contexts or multiple sessions


## Performance Tuning Strategy

### **Phase 1: Immediate Fixes (Expected 5-10x improvement)**

1. Add `-ngl -1` for full GPU offloading
2. Add `-fa` for Flash Attention
3. Add `-b 2048 -ub 512` for optimal batching

### **Phase 2: Memory Optimization (Saves 3GB VRAM)**

1. Add `--cache-type-k q8_0 --cache-type-v q8_0`
2. This enables larger context windows or multi-session capabilities

### **Phase 3: Fine-tuning (Additional 20-50% improvement)**

1. Optimize threading with `-t 4 -tb 8`
2. Enable continuous batching with `--cont-batching --parallel 2`
3. Scale context to `-c 16384` if needed

## Monitoring and Validation

After implementing these optimizations, monitor:

- **GPU utilization**: Should be >80% during inference (`nvidia-smi`)
- **VRAM usage**: Should stay under 15GB
- **Token generation**: Should achieve 15-25+ tokens/second
- **Model quality**: Should maintain high quality with Q8_0 cache quantization


## Architecture-Specific Considerations

Your **RTX A4000 (Ada Lovelace)** benefits specifically from:

- **Flash Attention**: Optimized for Ada Lovelace memory hierarchy[^1_2]
- **Tensor Core utilization**: Enhanced with proper GPU offloading
- **CUDA Graphs**: Automatically enabled with continuous batching
- **Memory bandwidth**: Maximized with quantized KV cache

The research shows that similar setups achieve **26-54 tokens/second** on comparable hardware, so your target of 15-25+ tokens/second is conservative and achievable.[^1_7]
<span style="display:none">[^1_10][^1_11][^1_12][^1_13][^1_14][^1_15][^1_16][^1_17][^1_18][^1_19][^1_20][^1_21][^1_22][^1_23][^1_24][^1_25][^1_26][^1_27][^1_28][^1_29][^1_30][^1_31][^1_32][^1_33][^1_34][^1_35][^1_36][^1_37][^1_38][^1_39][^1_40][^1_41][^1_42][^1_43][^1_44][^1_45][^1_46][^1_47][^1_48][^1_49][^1_50][^1_51][^1_52][^1_53][^1_54][^1_55][^1_56][^1_8][^1_9]</span>

<div style="text-align: center">‚ÅÇ</div>

[^1_1]: https://docs.redhat.com/en/documentation/red_hat_ai_inference_server/3.0/html/vllm_server_arguments/all-server-arguments-server-arguments

[^1_2]: https://www.reddit.com/r/LocalLLaMA/comments/1k3plzq/llama_4_slow_prompt_processing_on_llamacpp_with/

[^1_3]: https://www.edtechagent.ai/guides/llama3-optimization

[^1_4]: https://llama-cpp-python.readthedocs.io/en/latest/api-reference/

[^1_5]: https://developer.nvidia.com/blog/accelerating-llms-with-llama-cpp-on-nvidia-rtx-systems/

[^1_6]: https://github.com/abetlen/llama-cpp-python/issues/1123

[^1_7]: https://steelph0enix.github.io/posts/llama-cpp-guide/

[^1_8]: https://www.youtube.com/watch?v=JSDf0IGqwag

[^1_9]: https://discuss.huggingface.co/t/llama-7b-gpu-memory-requirement/34323

[^1_10]: https://github.com/ggml-org/llama.cpp

[^1_11]: https://forum.level1techs.com/t/full-deepseek-q1-with-the-ik-version-of-llama-cpp-on-am5-no-distills-just-a-quant/233530

[^1_12]: https://llama-stack.readthedocs.io/en/latest/references/llama_stack_client_cli_reference.html

[^1_13]: https://llama-cpp-python.readthedocs.io/en/latest/server/

[^1_14]: https://github.com/ggerganov/llama.cpp/issues/5993

[^1_15]: https://github.com/ggml-org/llama.cpp/discussions/8652

[^1_16]: https://www.reddit.com/r/LocalLLaMA/comments/14djns5/llamacpp_and_thread_count_optimization/

[^1_17]: https://llama-stack.readthedocs.io/en/latest/references/llama_cli_reference/index.html

[^1_18]: https://pyimagesearch.com/2024/08/26/llama-cpp-the-ultimate-guide-to-efficient-llm-inference-and-applications/

[^1_19]: https://learn.arm.com/learning-paths/servers-and-cloud-computing/llama-cpu/llama-chatbot/

[^1_20]: https://github.com/ggml-org/llama.cpp/discussions/15396

[^1_21]: https://www.reddit.com/r/LocalLLaMA/comments/1kca20r/feedback_on_my_llamacpp_docker_run_command_batch/

[^1_22]: https://www.reddit.com/r/LocalLLaMA/comments/1kmrfoo/mla_optimization_with_flashattention_for/

[^1_23]: https://github.com/gpustack/llama-box

[^1_24]: https://github.com/Dao-AILab/flash-attention

[^1_25]: https://docs.lmcache.ai/getting_started/quickstart/offload_kv_cache.html

[^1_26]: https://www.reddit.com/r/LocalLLaMA/comments/1kwdpey/best_settings_for_running_qwen330ba3b_with/

[^1_27]: https://github.com/ggml-org/llama.cpp/discussions/9646

[^1_28]: https://github.com/ggerganov/llama.cpp/issues/11200

[^1_29]: https://news.ycombinator.com/item?id=43743337

[^1_30]: https://smcleod.net/2024/12/bringing-k/v-context-quantisation-to-ollama/

[^1_31]: https://dai.sjtu.edu.cn/my_file/pdf/a43c4dc7-6132-49cb-b737-311eefde646f.pdf

[^1_32]: https://github.com/ggml-org/llama.cpp/discussions/9784

[^1_33]: https://github.com/microsoft/PhiCookBook/blob/main/md/01.Introduction/04/UsingLlamacppQuantifyingPhi.md

[^1_34]: https://www.reddit.com/r/LocalLLaMA/comments/1i2gvcg/compared_amd_7900_xtx_to_nvidia_ada_4000_sff/

[^1_35]: https://github.com/ggerganov/llama.cpp/issues/7885

[^1_36]: https://www.aimodels.fyi/models/huggingFace/phi-4-gguf-bartowski

[^1_37]: https://github.com/ggml-org/llama.cpp/discussions/2948

[^1_38]: https://training.continuumlabs.ai/infrastructure/data-and-memory/calculating-gpu-memory-for-serving-llms

[^1_39]: https://huggingface.co/Mungert/Phi-4-reasoning-plus-GGUF

[^1_40]: https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/nlp/nemo_megatron/batching.html

[^1_41]: https://huggingface.co/Mungert/Phi-4-mini-reasoning-GGUF

[^1_42]: https://jan.ai/docs/llama-cpp

[^1_43]: https://github.com/ggml-org/llama.cpp/issues/15136

[^1_44]: https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/

[^1_45]: https://buttondown.com/weekly-project-news/archive/weekly-github-report-for-llamacpp-april-21-2025/

[^1_46]: https://www.reddit.com/r/LocalLLaMA/comments/190v426/llamacpp_cpu_optimization/

[^1_47]: https://docs.llamaindex.ai/en/stable/CHANGELOG/

[^1_48]: https://pypi.org/project/llama-cpp-python-binary/

[^1_49]: https://en.wikipedia.org/wiki/Llama_(language_model)

[^1_50]: https://llama-cpp-python.readthedocs.io

[^1_51]: https://buttondown.com/weekly-project-news/archive/weekly-github-report-for-llamacpp-april-28-2025-4234/

[^1_52]: https://github.com/ggml-org/llama.cpp/discussions/8547

[^1_53]: https://www.reddit.com/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/

[^1_54]: https://python.langchain.com/docs/integrations/llms/llamacpp/

[^1_55]: https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html

[^1_56]: https://ppl-ai-code-interpreter-files.s3.amazonaws.com/web/direct-files/fd80d4ee502e731c08ed985a0e53eb35/408e0d0e-99c1-4ae6-93d4-079fab55fc21/dc032bcd.csv

