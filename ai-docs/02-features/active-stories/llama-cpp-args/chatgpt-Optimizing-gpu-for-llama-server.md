# Optimizing GPU inference with llama-server

To maximize tokens/sec for Phi-4 on an A4000 (16 GB), ensure llama.cpp actually uses the GPU and is configured for full parallelism. First, **verify device detection**: run llama-server \--list-devices to see the available CUDA devices[\[1\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28env%3A%20LLAMA_ARG_NUMA%29%20%7C%20%7C%20%60,none%3A%20use%20one%20GPU%20only). If not listed, install the CUDA toolkit and use a CUDA-enabled build. By default, llama.cpp will use all visible GPUs; you can control this with \--device. For a single GPU, you can omit \--device or set it to cuda0.

Next, **offload all model layers to the GPU**. Use the \--n-gpu-layers N (or \-ngl N) flag to specify how many transformer layers to keep in VRAM[\[1\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28env%3A%20LLAMA_ARG_NUMA%29%20%7C%20%7C%20%60,none%3A%20use%20one%20GPU%20only). For example, \-ngl 99 offloads *all* layers to the GPU (the exact number 99 is used as “all” in examples[\[2\]](https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html#:~:text=,For)). This ensures inference is done on the fast GPU memory instead of slower CPU memory. If you had multiple GPUs, you could also use \--split-mode row or \--tensor-split to shard layers across cards[\[2\]](https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html#:~:text=,For)[\[1\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28env%3A%20LLAMA_ARG_NUMA%29%20%7C%20%7C%20%60,none%3A%20use%20one%20GPU%20only), but for one A4000 use the default (no split).

Enable **Flash Attention** with \-fa (or \--flash-attn) to speed up attention on GPU[\[2\]](https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html#:~:text=,For)[\[3\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28env%3A%20LLAMA_ARG_UBATCH%29%20%7C%20%7C%20%60,perf%60%20%7C%20disable%20internal%20libllama). By default Flash Attention kernels are available if llama.cpp was built with CUDA support. In practice, include \-fa in your command to ensure it’s used. Additionally, set environment variables to exploit GPU optimizations: e.g. GGML\_CUDA\_F16=1 to use 16-bit (FP16) arithmetic (enabling tensor cores) and GGML\_CUDA\_FORCE\_CUBLAS=1 to use cuBLAS for faster matmuls[\[4\]](https://node-llama-cpp.withcat.ai/guide/CUDA#:~:text=,use%20peer%20to%20peer%20copies)[\[5\]](https://node-llama-cpp.withcat.ai/guide/CUDA#:~:text=kernels%20,ON). (These may require llama.cpp built with the corresponding compile flags, but setting them at runtime can trigger these modes if supported.)

**Disable KV offloading** if GPU memory allows: use \--no-kv-offload (\-nkvo) to keep the key/value cache on the GPU[\[6\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28env%3A%20LLAMA_ARG_YARN_BETA_FAST%29%20%7C%20%7C%20%60,offload%60%20%7C%20disable%20KV%20offload). This avoids GPU↔CPU transfers during decoding and greatly boosts throughput. By default llama.cpp may offload KV to RAM for very large contexts, but for a 14.7B model in Q4 on a 16GB GPU, it’s often safe to keep it on-GPU. You can also downcast KV precision if needed: by default KV is f16, but flags \--cache-type-k TYPE and \--cache-type-v TYPE let you use quantized types (e.g. q4\_0) to save memory[\[7\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28env%3A%20LLAMA_ARG_NO_KV_OFFLOAD%29%20%7C%20%7C%20%60,default%3A%20f16)[\[8\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28env%3A%20LLAMA_ARG_CACHE_TYPE_K%29%20%7C%20%7C%20%60,default%3A%20f16). For example, \--cache-type-k q4\_0 \--cache-type-v q4\_0 would quantize the KV cache to 4-bit (at minor quality cost) and further reduce VRAM usage if you run out of memory.

**Batching and threading:** Use large batch sizes to fully utilize the GPU. The llama-server flags \-b N (logical batch size) and \-ub N (physical, device-level batch size) control pipelining[\[9\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=,tokens%20to%20keep%20from%20the). For single-user throughput, you can often set them equal and large (e.g. \-b 2048 \-ub 2048 or higher) to feed many tokens per step into the GPU. By default the max batch is 2048 and ubatch is 512[\[9\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=,tokens%20to%20keep%20from%20the); increasing them (up to your context size) improves throughput. Similarly, allow llama.cpp to use multiple CPU threads for prompt processing by setting \--threads and \--threads-batch. The default \--threads \-1 uses all cores[\[10\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28default%3A%20false%29%20%7C%20%7C%20%60,high). You may explicitly set \--threads \<num\> and \--threads-batch \<num\> to your CPU count (e.g. 16\) to ensure full CPU utilization for any remaining CPU work. The \--parallel flag controls multi-sequence parallel decoding (usually 1 for single-user)[\[11\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28env%3A%20LLAMA_ARG_CACHE_TYPE_V%29%20%7C%20%7C%20%60,map%20model%20%28slower%20load) and can be left at 1 for highest single-stream speed.

**Context and generation options:** The context window (\--ctx-size) determines how many tokens of prompt/previous chat history the model sees. The default is 4096[\[9\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=,tokens%20to%20keep%20from%20the), which you can increase if you need longer context (but this slightly reduces throughput due to more KV per step). The \--predict (\-n) sets max tokens to generate; use a large number or \-1 (infinite) for full generation. Other sampling parameters (temperature, top‑k/p, etc.) should be set as needed for quality, but they minimally affect raw speed.

**Example optimal command:** Putting it all together, a tuned command might look like:

export GGML\_CUDA\_F16=1  GGML\_CUDA\_FORCE\_CUBLAS=1  
llama-server \\  
  \-m /models/phi-4-gguf \\  
  \--alias phi4 \\  
  \-c 4096 \-n 2048 \\  
  \-b 2048 \-ub 2048 \\  
  \--threads \-1 \--threads-batch \-1 \\  
  \-ngl 99 \-sm none \-fa \\  
  \--no-kv-offload \\  
  \--cache-type-k q4\_0 \--cache-type-v q4\_0 \\  
  \--api-key \<KEY\> \--jinja \--port 8080

This tells llama-server to use the Phi-4 model file, set a large context (4096) and generate \~2048 tokens, use all CPU threads, offload all 99 layers to GPU, enable Flash Attention, disable KV offload, and quantize KV to Q4. (Adjust \-b/\-ub or context as needed for your application.) With this config, the model should fully use the A4000’s CUDA cores and achieve much higher throughput.

**Sources:** The above flags and defaults are documented in llama.cpp’s server usage guide[\[9\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=,tokens%20to%20keep%20from%20the)[\[6\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28env%3A%20LLAMA_ARG_YARN_BETA_FAST%29%20%7C%20%7C%20%60,offload%60%20%7C%20disable%20KV%20offload), and GPU tips (use \-ngl and \-fa, etc.) are confirmed by users and documentation[\[2\]](https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html#:~:text=,For)[\[3\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28env%3A%20LLAMA_ARG_UBATCH%29%20%7C%20%7C%20%60,perf%60%20%7C%20disable%20internal%20libllama). In particular, \-ngl 99 offloads all layers to GPU[\[2\]](https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html#:~:text=,For), \-fa enables faster attention[\[2\]](https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html#:~:text=,For)[\[3\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28env%3A%20LLAMA_ARG_UBATCH%29%20%7C%20%7C%20%60,perf%60%20%7C%20disable%20internal%20libllama), and large \-b/-ub batches improve utilization[\[9\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=,tokens%20to%20keep%20from%20the). These settings should maximize tokens/sec on the A4000 for the given Phi-4 Q4 model.

---

[\[1\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28env%3A%20LLAMA_ARG_NUMA%29%20%7C%20%7C%20%60,none%3A%20use%20one%20GPU%20only) [\[3\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28env%3A%20LLAMA_ARG_UBATCH%29%20%7C%20%7C%20%60,perf%60%20%7C%20disable%20internal%20libllama) [\[6\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28env%3A%20LLAMA_ARG_YARN_BETA_FAST%29%20%7C%20%7C%20%60,offload%60%20%7C%20disable%20KV%20offload) [\[7\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28env%3A%20LLAMA_ARG_NO_KV_OFFLOAD%29%20%7C%20%7C%20%60,default%3A%20f16) [\[8\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28env%3A%20LLAMA_ARG_CACHE_TYPE_K%29%20%7C%20%7C%20%60,default%3A%20f16) [\[9\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=,tokens%20to%20keep%20from%20the) [\[10\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28default%3A%20false%29%20%7C%20%7C%20%60,high) [\[11\]](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md#:~:text=%28env%3A%20LLAMA_ARG_CACHE_TYPE_V%29%20%7C%20%7C%20%60,map%20model%20%28slower%20load) raw.githubusercontent.com

[https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md](https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md)

[\[2\]](https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html#:~:text=,For) llama.cpp \- Qwen

[https://qwen.readthedocs.io/en/latest/run\_locally/llama.cpp.html](https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html)

[\[4\]](https://node-llama-cpp.withcat.ai/guide/CUDA#:~:text=,use%20peer%20to%20peer%20copies) [\[5\]](https://node-llama-cpp.withcat.ai/guide/CUDA#:~:text=kernels%20,ON) CUDA Support | node-llama-cpp

[https://node-llama-cpp.withcat.ai/guide/CUDA](https://node-llama-cpp.withcat.ai/guide/CUDA)