import { ModelCatalog } from '@/app/ui/setup/download-models/types';

export const chatModelsCatalog: ModelCatalog[] = [
  {
    id: 'qwen2.5-14b',
    name: 'Qwen2.5 14B',
    repo: 'bartowski/Qwen2.5-14B-Instruct-GGUF',
    filename: 'Qwen2.5-14B-Instruct-Q4_K_M.gguf',
    quantization: 'Q4_K_M',
    size: '8.99GB',
    parameters: '14B',
    category: 'chat',
    tier: 'premium',
    badge: '⭐ Best Overall',
    ratings: { quality: 4.5, speed: 4, specialization: 4.5 },
    benchmarks: { mmlu: 79.7, bbb: 78.2 },
    contextWindow: '128K',
    memoryEstimate: '~8.99GB',
    license: 'Apache 2.0',
    licenseUrl: 'https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF',
    tooltipContent: {
      strengths: [
        'Exceptional general-purpose performance',
        'Strong math and coding',
        'Excellent JSON output generation',
      ],
      useCase: 'Best all-around model for general tasks, coding assistance, and structured outputs',
      researchNotes:
        'Trained on 18T tokens (up from 7T in Qwen2), outperforms Qwen2-57B-A14B. Beats competitors of larger sizes on MMLU/BBH benchmarks.',
    },
  },
  {
    id: 'phi-4-14b',
    name: 'Phi-4 14B',
    repo: 'bartowski/phi-4-GGUF',
    filename: 'phi-4-Q4_K_M.gguf',
    quantization: 'Q4_K_M',
    size: '9.05GB',
    parameters: '14B',
    category: 'chat',
    tier: 'premium',
    badge: '⭐ Best Reasoning',
    ratings: { quality: 5, speed: 4, specialization: 5 },
    benchmarks: { mmlu: 84.8, humanEval: 82.6 },
    contextWindow: '16K',
    memoryEstimate: '~9.05GB',
    license: 'MIT',
    licenseUrl: 'https://huggingface.co/bartowski/Phi-4-GGUF',
    tooltipContent: {
      strengths: [
        'Best-in-class math and reasoning',
        'Excellent coding capabilities',
        'Outperforms GPT-4 on GPQA by 6 points',
      ],
      useCase: 'Ideal for complex reasoning, mathematical problems, and advanced coding tasks',
      researchNotes:
        'Small Language Model (SLM) trained on 9.8T high-quality tokens over 21 days using 1920 H100 GPUs. Demonstrates that smaller, well-trained models can compete with much larger ones.',
    },
  },
  {
    id: 'qwen2.5-32b',
    name: 'Qwen2.5 32B',
    repo: 'bartowski/Qwen2.5-32B-Instruct-GGUF',
    filename: 'Qwen2.5-32B-Instruct-Q4_K_M.gguf',
    quantization: 'Q4_K_M',
    size: '19.85GB',
    parameters: '32B',
    category: 'chat',
    tier: 'premium',
    badge: '⭐ Best Advanced',
    ratings: { quality: 5, speed: 3.5, specialization: 5 },
    benchmarks: { mmlu: 84.5 },
    contextWindow: '128K',
    memoryEstimate: '~19.85GB (tight fit)',
    license: 'Apache 2.0',
    licenseUrl: 'https://huggingface.co/bartowski/Qwen2.5-32B-Instruct-GGUF',
    tooltipContent: {
      strengths: ['Superior math and coding performance', 'Beats many 72B models', 'Excellent structured outputs'],
      useCase: 'Advanced tasks requiring maximum capability within 16GB Mac constraints (tight memory fit)',
      researchNotes:
        'MATH: 57.7, MBPP: 84.5. Significantly outperforms its predecessor Qwen1.5-32B, especially in mathematics and coding. Requires most available memory.',
    },
  },
  {
    id: 'mistral-nemo-12b',
    name: 'Mistral NeMo 12B',
    repo: 'bartowski/Mistral-Nemo-Instruct-2407-GGUF',
    filename: 'Mistral-Nemo-Instruct-2407-Q4_K_M.gguf',
    quantization: 'Q4_K_M',
    size: '7.48GB',
    parameters: '12B',
    category: 'chat',
    tier: 'specialized',
    badge: 'Long Context',
    ratings: { quality: 4, speed: 5, specialization: 5 },
    benchmarks: {},
    contextWindow: '128K',
    memoryEstimate: '~7.48GB',
    license: 'Apache 2.0',
    licenseUrl: 'https://huggingface.co/bartowski/Mistral-Nemo-Instruct-2407-GGUF',
    tooltipContent: {
      strengths: ['128K context window', 'Fastest inference (154 tokens/s)', 'FP8 quantization support'],
      useCase: 'Best for long documents, large context windows, and speed-critical applications',
      researchNotes:
        'Built with NVIDIA, surpasses Gemma 2 9B and Llama 3 8B. Uses Tekken tokenizer for improved multilingual compression. Excellent for long-context applications.',
    },
  },
  {
    id: 'gemma-3-12b',
    name: 'Gemma 3 12B',
    repo: 'lmstudio-community/gemma-3-12b-it-GGUF',
    filename: 'gemma-3-12b-it-Q4_K_M.gguf',
    quantization: 'Q4_K_M',
    size: '7.3GB',
    parameters: '12B',
    category: 'chat',
    tier: 'specialized',
    badge: 'Multimodal Latest',
    ratings: { quality: 4.5, speed: 4, specialization: 5 },
    benchmarks: {},
    contextWindow: '128K',
    memoryEstimate: '~7.3GB',
    license: 'Gemma License',
    licenseUrl: 'https://huggingface.co/lmstudio-community/gemma-3-12b-it-GGUF',
    tooltipContent: {
      strengths: ['Multimodal vision support', '35+ languages', 'March 2025 release', 'Function calling'],
      useCase: 'Best for multilingual tasks, vision-language inputs, and latest Google Gemini 2.0 technology',
      researchNotes:
        'Trained on 12T tokens. Outperforms Llama3-405B and DeepSeek-V3 in human preference evaluations. Supports vision-language input with text outputs.',
    },
  },
  {
    id: 'gpt-oss-20b',
    name: 'GPT-OSS 20B',
    repo: 'ggml-org/gpt-oss-20b-GGUF',
    filename: 'gpt-oss-20b-mxfp4.gguf',
    quantization: 'Q4_K_M',
    size: '12.1GB',
    parameters: '20B',
    category: 'chat',
    tier: 'specialized',
    badge: 'OpenAI Open-Weight',
    ratings: { quality: 4, speed: 3.5, specialization: 4 },
    benchmarks: { mmlu: 69, humanEval: 87.3 },
    contextWindow: '16K',
    memoryEstimate: '~12.1GB',
    license: 'Apache 2.0',
    licenseUrl: 'https://huggingface.co/ggml-org/gpt-oss-20b-GGUF',
    tooltipContent: {
      strengths: [
        'OpenAI first open-weight GPT model',
        'MoE architecture (20.9B total, 3.6B active)',
        'Strong code generation',
      ],
      useCase: 'For those wanting OpenAI architecture with open-source license',
      researchNotes:
        'Released August 2025. Surprisingly outperformed gpt-oss-120B on MMLU (69% vs 66%). Good balance of performance and efficiency with MoE architecture.',
    },
  },
];

export const embeddingModelsCatalog: ModelCatalog[] = [
  {
    id: 'qwen3-embedding-4b',
    name: 'Qwen3 Embedding 4B',
    repo: 'Qwen/Qwen3-Embedding-4B-GGUF',
    filename: 'Qwen3-Embedding-4B-Q4_K_M.gguf',
    quantization: 'Q4_K_M',
    size: '2.5GB',
    parameters: '4B',
    category: 'embedding',
    tier: 'premium',
    badge: '⭐ Top Choice',
    ratings: { quality: 5, speed: 4, specialization: 5 },
    benchmarks: { mteb: 70.58 },
    contextWindow: '8K',
    memoryEstimate: '~2.5GB',
    license: 'Apache 2.0',
    licenseUrl: 'https://huggingface.co/Qwen/Qwen3-Embedding-4B-GGUF',
    tooltipContent: {
      strengths: ['#1 on MTEB multilingual leaderboard', '100+ languages', 'Available in GGUF format'],
      useCase: 'Best overall embedding model for RAG applications and semantic search',
      researchNotes:
        '8B variant ranks #1 on MTEB multilingual leaderboard (score: 70.58 as of June 2025). Excellent for multilingual use cases with llama.cpp compatibility.',
    },
  },
  {
    id: 'nomic-embed-v1.5',
    name: 'Nomic Embed v1.5',
    repo: 'nomic-ai/nomic-embed-text-v1.5-GGUF',
    filename: 'nomic-embed-text-v1.5.Q8_0.gguf',
    quantization: 'Q8_0',
    size: '274MB',
    parameters: '137M',
    category: 'embedding',
    tier: 'specialized',
    badge: 'Most Efficient',
    ratings: { quality: 4, speed: 5, specialization: 5 },
    benchmarks: {},
    contextWindow: '8K',
    memoryEstimate: '~274MB',
    license: 'Apache 2.0',
    licenseUrl: 'https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF',
    tooltipContent: {
      strengths: [
        'Ultra-small size (137M params)',
        'Matryoshka learning (64-768 dims)',
        'Excellent speed',
        '8K context',
      ],
      useCase: 'Best for resource-constrained scenarios and speed-critical applications',
      researchNotes:
        'Ranks similarly to top-10 MTEB models that are 70x bigger. Supports flexible embedding dimensions (64-768) for size/performance tradeoff. 86.2% top-5 accuracy.',
    },
  },
  {
    id: 'bge-large-en-v1.5',
    name: 'BGE Large EN v1.5',
    repo: 'mradermacher/bge-large-en-v1.5-GGUF',
    filename: 'bge-large-en-v1.5.Q4_K_M.gguf',
    quantization: 'Q4_K_M',
    size: '208MB',
    parameters: '335M',
    category: 'embedding',
    tier: 'specialized',
    badge: 'Strong English',
    ratings: { quality: 4.5, speed: 4.5, specialization: 5 },
    benchmarks: {},
    contextWindow: '512',
    memoryEstimate: '~208MB',
    license: 'MIT',
    licenseUrl: 'https://huggingface.co/mradermacher/bge-large-en-v1.5-GGUF',
    tooltipContent: {
      strengths: ['Excellent English performance', '1024 dimensions', 'Strong BAAI pedigree', '84.7% accuracy'],
      useCase: 'Optimal for English-only RAG applications requiring high accuracy',
      researchNotes:
        'Part of BAAI General Embedding (BGE) family. BGE edges ahead in raw accuracy (84.7%) and transforms text into 1024-dimensional vectors for dense retrieval.',
    },
  },
];
