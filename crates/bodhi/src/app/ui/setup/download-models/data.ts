import { ModelCatalog } from '@/app/ui/setup/download-models/types';

export const chatModelsCatalog: ModelCatalog[] = [
  {
    id: 'nemotron-nano-3-30b',
    name: 'Nemotron Nano 3 30B',
    repo: 'ggml-org/Nemotron-Nano-3-30B-A3B-GGUF',
    filename: 'Nemotron-Nano-3-30B-A3B-Q4_K_M.gguf',
    quantization: 'Q4_K_M',
    size: '24.5GB',
    parameters: '30B (3B active)',
    category: 'chat',
    tier: 'premium',
    badge: '⭐ Best Overall',
    ratings: { quality: 5, speed: 4.5, specialization: 5 },
    benchmarks: {},
    contextWindow: '128K',
    memoryEstimate: '~24.5GB',
    license: 'Apache 2.0',
    licenseUrl: 'https://huggingface.co/ggml-org/Nemotron-Nano-3-30B-A3B-GGUF',
    tooltipContent: {
      strengths: ['NVIDIA MoE architecture (3B active params)', 'Coding benchmark leader', 'Excellent efficiency'],
      useCase: 'Best all-around model for general tasks, exceptional coding capabilities',
      researchNotes: "NVIDIA's mixture-of-experts model with only 3B activated parameters. Leads LiveCodeBench.",
    },
  },
  {
    id: 'phi-4-reasoning',
    name: 'Phi-4 Reasoning',
    repo: 'bartowski/microsoft_Phi-4-reasoning-GGUF',
    filename: 'Phi-4-reasoning-Q8_0.gguf',
    quantization: 'Q8_0',
    size: '15.6GB',
    parameters: '14B',
    category: 'chat',
    tier: 'premium',
    badge: '⭐ Best Reasoning',
    ratings: { quality: 5, speed: 4, specialization: 5 },
    benchmarks: {},
    contextWindow: '16K',
    memoryEstimate: '~15.6GB',
    license: 'MIT',
    licenseUrl: 'https://huggingface.co/bartowski/microsoft_Phi-4-reasoning-GGUF',
    tooltipContent: {
      strengths: ['Most robust reasoning model', 'Competes with 5-50x larger models', 'Exceptional math capabilities'],
      useCase: 'Ideal for complex reasoning, mathematical problems, and tasks requiring consistency',
      researchNotes: "Microsoft's reasoning-optimized SLM. 82.2% robustness on linguistic variation tests.",
    },
  },
  {
    id: 'qwen3-32b',
    name: 'Qwen3 32B',
    repo: 'ggml-org/Qwen3-32B-GGUF',
    filename: 'Qwen3-32B-Q4_K_M.gguf',
    quantization: 'Q4_K_M',
    size: '19.8GB',
    parameters: '32B',
    category: 'chat',
    tier: 'premium',
    badge: '⭐ Best Advanced',
    ratings: { quality: 5, speed: 3.5, specialization: 5 },
    benchmarks: { humanEval: 91.0 },
    contextWindow: '128K',
    memoryEstimate: '~19.8GB',
    license: 'Apache 2.0',
    licenseUrl: 'https://huggingface.co/ggml-org/Qwen3-32B-GGUF',
    tooltipContent: {
      strengths: ['40% better than Qwen2.5', 'Trained on 36T tokens', '92.3% AIME 2025'],
      useCase: 'Advanced tasks requiring maximum capability, strong math/coding',
      researchNotes: 'Major upgrade from Qwen2.5. 36T tokens (2x predecessor). 15% fewer hallucinations.',
    },
  },
  {
    id: 'mistral-small-3.2',
    name: 'Mistral Small 3.2',
    repo: 'unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF',
    filename: 'Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M.gguf',
    quantization: 'Q4_K_M',
    size: '14.3GB',
    parameters: '24B',
    category: 'chat',
    tier: 'specialized',
    badge: 'Long Context',
    ratings: { quality: 4.5, speed: 4, specialization: 5 },
    benchmarks: {},
    contextWindow: '128K',
    memoryEstimate: '~14.3GB',
    license: 'Apache 2.0',
    licenseUrl: 'https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF',
    tooltipContent: {
      strengths: ['128K context window', 'Vision understanding support', 'Fits single RTX 4090 or 32GB Mac'],
      useCase: 'Best for long documents, multimodal tasks, and vision-language applications',
      researchNotes: 'June 2025 release. State-of-the-art vision understanding with long context.',
    },
  },
  {
    id: 'glm-4.6v-flash',
    name: 'GLM-4.6V Flash',
    repo: 'ggml-org/GLM-4.6V-Flash-GGUF',
    filename: 'GLM-4.6V-Flash-Q8_0.gguf',
    quantization: 'Q8_0',
    size: '10GB',
    parameters: '9B',
    category: 'chat',
    tier: 'specialized',
    badge: 'Multimodal Latest',
    ratings: { quality: 4.5, speed: 4, specialization: 5 },
    benchmarks: {},
    contextWindow: '128K',
    memoryEstimate: '~10GB',
    license: 'Apache 2.0',
    licenseUrl: 'https://huggingface.co/ggml-org/GLM-4.6V-Flash-GGUF',
    tooltipContent: {
      strengths: ['Native vision support', 'Native tool calling', 'December 2025 release', '128K context'],
      useCase: 'Best for vision-language tasks, tool use, and multimodal applications',
      researchNotes: "Zhipu AI's latest vision model. 128K context with native tool calling support.",
    },
  },
  {
    id: 'gpt-oss-20b',
    name: 'GPT-OSS 20B',
    repo: 'ggml-org/gpt-oss-20b-GGUF',
    filename: 'gpt-oss-20b-mxfp4.gguf',
    quantization: 'Q4_K_M',
    size: '12.1GB',
    parameters: '20B',
    category: 'chat',
    tier: 'specialized',
    badge: 'OpenAI Open-Weight',
    ratings: { quality: 4, speed: 3.5, specialization: 4 },
    benchmarks: { mmlu: 69, humanEval: 87.3 },
    contextWindow: '16K',
    memoryEstimate: '~12.1GB',
    license: 'Apache 2.0',
    licenseUrl: 'https://huggingface.co/ggml-org/gpt-oss-20b-GGUF',
    tooltipContent: {
      strengths: [
        'OpenAI first open-weight GPT model',
        'MoE architecture (20.9B total, 3.6B active)',
        'Strong code generation',
      ],
      useCase: 'For those wanting OpenAI architecture with open-source license',
      researchNotes:
        'Released August 2025. Surprisingly outperformed gpt-oss-120B on MMLU (69% vs 66%). Good balance of performance and efficiency with MoE architecture.',
    },
  },
];

export const embeddingModelsCatalog: ModelCatalog[] = [
  {
    id: 'qwen3-embedding-8b',
    name: 'Qwen3 Embedding 8B',
    repo: 'Qwen/Qwen3-Embedding-8B-GGUF',
    filename: 'Qwen3-Embedding-8B-Q8_0.gguf',
    quantization: 'Q8_0',
    size: '8.05GB',
    parameters: '8B',
    category: 'embedding',
    tier: 'premium',
    badge: '⭐ Top Choice',
    ratings: { quality: 5, speed: 4, specialization: 5 },
    benchmarks: { mteb: 70.58 },
    contextWindow: '8K',
    memoryEstimate: '~8.05GB',
    license: 'Apache 2.0',
    licenseUrl: 'https://huggingface.co/Qwen/Qwen3-Embedding-8B-GGUF',
    tooltipContent: {
      strengths: ['#1 on MTEB multilingual leaderboard', '100+ languages', 'Upgraded from 4B'],
      useCase: 'Best overall embedding model for RAG applications and semantic search',
      researchNotes: '#1 MTEB multilingual (70.58 as of Jan 2026). Significant upgrade from 4B variant.',
    },
  },
  {
    id: 'bge-m3',
    name: 'BGE-M3',
    repo: 'gpustack/bge-m3-GGUF',
    filename: 'bge-m3-Q4_K_M.gguf',
    quantization: 'Q4_K_M',
    size: '438MB',
    parameters: '567M',
    category: 'embedding',
    tier: 'premium',
    badge: '⭐ Best Multilingual',
    ratings: { quality: 5, speed: 4, specialization: 5 },
    benchmarks: { mteb: 'Top Performer' },
    contextWindow: '8K',
    memoryEstimate: '~438MB',
    license: 'MIT',
    licenseUrl: 'https://huggingface.co/gpustack/bge-m3-GGUF',
    tooltipContent: {
      strengths: [
        'Best multilingual performance',
        'Supports 100+ languages',
        'Multi-functionality (dense, sparse, multi-vector)',
        '8K context window',
      ],
      useCase: 'Ideal for multilingual RAG applications and cross-lingual retrieval',
      researchNotes:
        'State-of-the-art multilingual embedding model from BAAI. Supports multi-functionality, multi-linguality, and multi-granularity with input sizes up to 8192 tokens.',
    },
  },
  {
    id: 'nomic-embed-v1.5',
    name: 'Nomic Embed v1.5',
    repo: 'nomic-ai/nomic-embed-text-v1.5-GGUF',
    filename: 'nomic-embed-text-v1.5.Q8_0.gguf',
    quantization: 'Q8_0',
    size: '274MB',
    parameters: '137M',
    category: 'embedding',
    tier: 'specialized',
    badge: 'Most Efficient',
    ratings: { quality: 4, speed: 5, specialization: 5 },
    benchmarks: {},
    contextWindow: '8K',
    memoryEstimate: '~274MB',
    license: 'Apache 2.0',
    licenseUrl: 'https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF',
    tooltipContent: {
      strengths: [
        'Ultra-small size (137M params)',
        'Matryoshka learning (64-768 dims)',
        'Excellent speed',
        '8K context',
      ],
      useCase: 'Best for resource-constrained scenarios and speed-critical applications',
      researchNotes:
        'Ranks similarly to top-10 MTEB models that are 70x bigger. Supports flexible embedding dimensions (64-768) for size/performance tradeoff. 86.2% top-5 accuracy.',
    },
  },
  {
    id: 'bge-large-en-v1.5',
    name: 'BGE Large EN v1.5',
    repo: 'mradermacher/bge-large-en-v1.5-GGUF',
    filename: 'bge-large-en-v1.5.Q4_K_M.gguf',
    quantization: 'Q4_K_M',
    size: '208MB',
    parameters: '335M',
    category: 'embedding',
    tier: 'specialized',
    badge: 'Strong English',
    ratings: { quality: 4.5, speed: 4.5, specialization: 5 },
    benchmarks: {},
    contextWindow: '512',
    memoryEstimate: '~208MB',
    license: 'MIT',
    licenseUrl: 'https://huggingface.co/mradermacher/bge-large-en-v1.5-GGUF',
    tooltipContent: {
      strengths: ['Excellent English performance', '1024 dimensions', 'Strong BAAI pedigree', '84.7% accuracy'],
      useCase: 'Optimal for English-only RAG applications requiring high accuracy',
      researchNotes:
        'Part of BAAI General Embedding (BGE) family. BGE edges ahead in raw accuracy (84.7%) and transforms text into 1024-dimensional vectors for dense retrieval.',
    },
  },
];
